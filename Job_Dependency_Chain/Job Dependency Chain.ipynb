{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a40066c",
   "metadata": {},
   "source": [
    "# Job Dependency Chain on HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3aff9",
   "metadata": {},
   "source": [
    "### Step 1. Get the parent JobID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05a5f8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "jid=$(sbatch --parsable --array=... my_array_script.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083666dd",
   "metadata": {},
   "source": [
    "\t•\t--parsable makes sbatch print only one token: the parent JobID of the array job (e.g. 123456).  \n",
    "\t•\tAll array tasks will be named 123456_1, 123456_2, …, but you only need the parent ID to set dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cba21",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Step 2.\tSubmit the next job with a dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d45ed3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sbatch --dependency=afterok:${jid} next_step.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc8fd6",
   "metadata": {},
   "source": [
    "\t•\tafterok:<JobID> tells Slurm: “start this job only after the referenced job and all its array tasks exit with state COMPLETED.”\n",
    "\t•\tIf any array task fails (FAILED, CANCELLED…), the dependency is not satisfied and the next job stays pending.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ddd4b",
   "metadata": {},
   "source": [
    "### Step 3.\tChain as many steps as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235578f",
   "metadata": {},
   "source": [
    "Store each parent JobID in a variable and pass it to the next submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8ec74",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "prev=\"\"                          # previous step\n",
    "for step in 1 2 3; do\n",
    "    jid=$(sbatch --parsable $([[ -n $prev ]] && echo \"--dependency=afterok:$prev\") step_$step.sh)\n",
    "    prev=$jid\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398db91",
   "metadata": {},
   "source": [
    "Slurm now enforces a dependency chain (aka chained jobs / workflow):  \n",
    "step 1 → step 2 → step 3, automatically handling all waiting and ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b54c17",
   "metadata": {},
   "source": [
    "## Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270bca4",
   "metadata": {},
   "source": [
    "Recently, I have been running a Wilcoxon test on a very large dataset, which did not finish even after 30 days. Therefore, I decided to split the entire dataset into smaller subsets and run the Wilcoxon tests on them in parallel. However, loading the dataset requires a node with 750 GB of memory, and such nodes are limited. Hence, I first need to perform the splitting step on a large-memory node and save the resulting subsets. After that, I can request hundreds of regular-memory nodes and run many Wilcoxon tests in parallel.\n",
    "\n",
    "Here, however, I encountered another problem. Each subset is about 6 GB in size, and there are around 16,000 subsets in total, which obviously exceeds the available storage capacity. Therefore, I decided to process them in groups of 200 subsets at a time. For each round, I extract 200 subsets from the original dataset, run the Wilcoxon tests on them, and then delete those 200 subsets to free up storage before proceeding to the next round. \n",
    "\n",
    "Even though I only process 200 subsets at a time, there are still more than 80 rounds in total. For each round, I need to first submit a split job to generate the 200 subsets, wait for it to finish and save them, and then submit another Wilcoxon job to analyze those subsets. This process is quite time-consuming.\n",
    "\n",
    "If I submit all the jobs to Slurm at once, there’s another problem — the next split job might start before the previous Wilcoxon job finishes, which would delete the subsets that are still being used by the unfinished Wilcoxon tasks. So I must wait until all 200 Wilcoxon tests from the previous round are completely finished before deleting those subsets and creating the next batch of subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48189944",
   "metadata": {},
   "source": [
    "To achieve this, I learned about the Job Dependency Chain method on GPT, which allows me to link multiple Slurm jobs together so that each step automatically starts only after the previous one has successfully completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfac69",
   "metadata": {},
   "source": [
    "### This script is a Slurm automation (driver) script designed to sequentially submit and manage multiple rounds of data processing tasks on a computing cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c865fab",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd /dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/\n",
    "\n",
    "split_script=/dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/huang_split_qiruiz_auto.sh\n",
    "wilcoxon_script=/dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/huang_wilcoxon_qiruiz.sh\n",
    "\n",
    "prev_dep=\"\"\n",
    "\n",
    "for x in {1..5}; do\n",
    "  if [[ -n \"$prev_dep\" ]]; then\n",
    "      dep_opt=\"--dependency=afterok:${prev_dep}\"\n",
    "  else\n",
    "      dep_opt=\"\"\n",
    "  fi\n",
    "\n",
    "  jid_split=$(sbatch --parsable $dep_opt \\\n",
    "                     --export=IDX=$x \"$split_script\")\n",
    "  echo \"[+] Split $x  →  JobID $jid_split\"\n",
    "\n",
    "  beg=$((200*(x-1)+1))\n",
    "  end=$((200*x))\n",
    "\n",
    "  jid_wil=$(sbatch --parsable --array=${beg}-${end} \\\n",
    "                   --dependency=afterok:${jid_split} \\\n",
    "                   \"$wilcoxon_script\")\n",
    "  echo \"    ↳ Wilcoxon ${beg}-${end}  →  JobID $jid_wil\"\n",
    "\n",
    "  prev_dep=$jid_wil\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98453dcc",
   "metadata": {},
   "source": [
    "### Here is the split_script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759aac2",
   "metadata": {},
   "source": [
    "/dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/huang_split_qiruiz_auto.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de720b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=zqr_huang_split\n",
    "#SBATCH --output=/dartfs/rc/lab/S/Szhao/qiruiz/perturb-predict-apply/task/result/huang_split_%A_%x_%j.out\n",
    "#SBATCH --time=06:00:00             \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=nccc\n",
    "#SBATCH --ntasks=1                     \n",
    "#SBATCH --cpus-per-task=1  \n",
    "#SBATCH --mem=750G \n",
    "#SBATCH --mail-user=f0070pp@dartmouth.edu\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL          \n",
    "\n",
    "idx=${IDX:-$1}\n",
    "if [[ -z \"$idx\" ]]; then\n",
    "  echo \"ERROR: IDX (split index) not provided; exiting.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "echo \"$SLURM_JOB_ID  starting split $idx  at $(date) on $(hostname)\"\n",
    "\n",
    "source /optnfs/common/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate gears\n",
    "\n",
    "rm -rf /dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/sc/Huang-HCT116/*\n",
    "\n",
    "cd /dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data\n",
    "python -u huang_parallel_split_adata.py \"$idx\"\n",
    "\n",
    "echo \"Split $idx completed\"\n",
    "echo \"Finished execution at: $(date)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70166245",
   "metadata": {},
   "source": [
    "### Here is the wilcoxon_script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b137e",
   "metadata": {},
   "source": [
    "/dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/huang_wilcoxon_qiruiz.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09960af1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=zqr_wilcoxon\n",
    "#SBATCH --output=/dartfs/rc/lab/S/Szhao/qiruiz/perturb-predict-apply/task/result/huang_wilcoxon_%A_%a.out\n",
    "#SBATCH --time=02:00:00             \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=nccc        \n",
    "#SBATCH --ntasks=1                     \n",
    "#SBATCH --cpus-per-task=1  \n",
    "#SBATCH --mem=24G \n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=f0070pp@dartmouth.edu\n",
    "          \n",
    "\n",
    "echo $SLURM_JOB_ID starting execution `date` on `hostname`\n",
    "\n",
    "source /software/python-anaconda-2022.05-el8-x86_64/etc/profile.d/conda.sh\n",
    "conda activate gears_cpu\n",
    "\n",
    "cd /dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data\n",
    "\n",
    "python -u huang_parallel_huang_wilcoxon.py ${SLURM_ARRAY_TASK_ID}\n",
    "echo \"Finished execution at: `date`\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34041feb",
   "metadata": {},
   "source": [
    "It automatically controls the execution order of two stages:  \n",
    "\t1.\tSplit stage  \n",
    "Runs huang_split_qiruiz_auto.sh on a large-memory node to divide the original big dataset into smaller subsets (200 per round).  \n",
    "\t2.\tWilcoxon stage  \n",
    "Runs huang_wilcoxon_qiruiz.sh on regular-memory nodes to perform Wilcoxon tests in parallel on those 200 subsets.  \n",
    "\n",
    "Using a Slurm Job Dependency Chain, the script ensures that the jobs run strictly in the correct sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee79eef",
   "metadata": {},
   "source": [
    "split(1) → wilcoxon(1–200)  \n",
    "       ↓  \n",
    "split(2) → wilcoxon(201–400)  \n",
    "       ↓  \n",
    "split(3) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589106a",
   "metadata": {},
   "source": [
    "### To submit the jobs, you only need to run the master.sh script on the terminal of the login node (the first script among the three).\n",
    "### bash master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0965f46",
   "metadata": {},
   "source": [
    "Then it will appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4423d",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "[f0070pp@discovery-01 ~]$ bash /dartfs/rc/lab/S/Szhao/perturbation_prediction/SeqExpDesign/vcc/data/huang_master.sh  \n",
    "[+] Split 1  →  JobID 6292666  \n",
    "    ↳ Wilcoxon 1-200  →  JobID 6292667  \n",
    "[+] Split 2  →  JobID 6292668  \n",
    "    ↳ Wilcoxon 201-400  →  JobID 6292669  \n",
    "[f0070pp@discovery-01 ~]$ squeue -u f0070pp  \n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)  \n",
    " 6292669_[201-400]  standard zqr_wilc  f0070pp PD       0:00      1 (Dependency)  \n",
    "           6292668  standard zqr_huan  f0070pp PD       0:00      1 (Dependency)  \n",
    "   6292667_[1-200]  standard zqr_wilc  f0070pp PD       0:00      1 (Dependency)  \n",
    "           6292666  standard zqr_huan  f0070pp PD       0:00      1 (Priority)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ee428",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\t1.\tOn Discovery, the largest available node has 750 GB of memory, but the pending time is very long and the node is unstable — sometimes the job freezes halfway without error.  \n",
    "\t2.\tThe total number of jobs that can be submitted at once through a Job Dependency Chain is also limited. On RCC, I encountered a limit of around 1,000 jobs (5 loops × 200 array tasks). However, even with this limitation, it is still much more convenient than submitting each round manually. I haven’t tested the single-submission limit on Discovery yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b50005",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "#### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"11\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"11\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"11\" # export NUMEXPR_NUM_THREADS=6\n",
    "os.environ[\"NUMBA_CACHE_DIR\"]='/tmp/numba_cache'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "\n",
    "import pickle\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "import numba\n",
    "import multiprocessing\n",
    "# njobs = max(1, multiprocessing.cpu_count())\n",
    "njobs = min(1, multiprocessing.cpu_count())\n",
    "numba.set_num_threads(njobs)\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "print(pd.__version__, sc.__version__, ad.__version__)\n",
    "\n",
    "\n",
    "def summarize(adata):\n",
    "    N_C, N_G = adata.shape\n",
    "    N_P = adata.obs['perturbation'].nunique()\n",
    "    N_P_2 = adata.obs['perturbation'][adata.obs['perturbation'].str.contains('_')].nunique()\n",
    "    return N_C, N_G, N_P, N_P - N_P_2, N_P_2\n",
    "\n",
    "def comp_bulk_expressions(adata, key='perturbation'):\n",
    "    '''\n",
    "    Calculate the bulk expressions (in the log-scale) in the given DataFrame.\n",
    "    This function groups the DataFrame by the 'perturbation' column and then \n",
    "    applies a transformation to calculate the average effect. The transformation \n",
    "    involves taking logarithm of the mean of the exponentiated values minus one for each group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData or pandas.DataFrame\n",
    "        An AnnData or a DataFrame containing the data with a column 'key'.\n",
    "    key : str, optional\n",
    "        The column name to group by, default is 'condition'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata_bulk : anndata.AnnData or pandas.DataFrame\n",
    "        An AnnData or a DataFrame with the average effect of each perturbation.\n",
    "    '''\n",
    "    if isinstance(adata, ad.AnnData):\n",
    "        df = adata.to_df()\n",
    "        key_pert = adata.obs[key]        \n",
    "    else:\n",
    "        df = adata\n",
    "        key_pert = key\n",
    "    obs = adata.obs.drop_duplicates(subset=[key]).set_index(key).sort_values(key)\n",
    "    obs['n_cells'] = adata.obs.groupby(key, observed=True).size()\n",
    "    var = adata.var\n",
    "    # save memory by removing the original adata\n",
    "    # del adata\n",
    "    gc.collect()\n",
    "    df = df.astype(np.float32)  # Ensure consistent data type for calculations\n",
    "    grouped = df.groupby(key_pert, observed=True)\n",
    "    sums = grouped.sum().astype(np.float32)\n",
    "    sizes = grouped.size().astype(np.float32)\n",
    "    means = sums / sizes.values[:, None]\n",
    "    stds = grouped.std().astype(np.float32)\n",
    "    adata_bulk = means  # You can return or use both means and stds as needed\n",
    "    del df\n",
    "    gc.collect()\n",
    "    adata_bulk = ad.AnnData(\n",
    "        X=sp.sparse.csr_matrix(adata_bulk.values), \n",
    "        obs=obs, \n",
    "        var=var\n",
    "        )\n",
    "    return adata_bulk, stds\n",
    "\n",
    "def comp_bulk_expressions_batch(adata, key='perturbation', group_size=1000):\n",
    "    \"\"\"\n",
    "    Wrapper function to calculate bulk expressions in smaller groups to save memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        An AnnData object containing the data with a column 'key'.\n",
    "    key : str, optional\n",
    "        The column name to group by, default is 'perturbation'.\n",
    "    group_size : int, optional\n",
    "        Number of perturbation conditions to process in each group, default is 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata_bulk : anndata.AnnData\n",
    "        An AnnData object with the average effect of each perturbation.\n",
    "    \"\"\"\n",
    "    perturbations = np.sort(np.array(adata.obs[key].unique()))\n",
    "    grouped_bulk = []\n",
    "    stds = []\n",
    "    for i in tqdm(range(0, len(perturbations), group_size), desc='Processing perturbations'):\n",
    "        subset_perturbations = perturbations[i:(i+group_size)]\n",
    "        sub_adata = adata[adata.obs[key].isin(subset_perturbations)]\n",
    "        if sub_adata.shape[0] == 0:\n",
    "            continue\n",
    "        sub_bulk, std = comp_bulk_expressions(sub_adata, key=key)\n",
    "        grouped_bulk.append(sub_bulk.copy())        \n",
    "        stds.append(std)\n",
    "    # Concatenate all sub-bulk results\n",
    "    adata_bulk = ad.concat(grouped_bulk, merge='same')\n",
    "    stds = pd.concat(stds, axis=0)\n",
    "    adata_bulk.uns['std'] = stds.copy()\n",
    "    return adata_bulk\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    # single perturbation\n",
    "    \"Adamson\", \"Frangieh\",    \n",
    "    \"Replogle-GW-k562\", \"Replogle-E-k562\", \"Replogle-E-rpe1\",\n",
    "    \"Tian-crispra\", \"Tian-crispri\",\n",
    "    \"Jiang-IFNB\", \"Jiang-IFNG\", \"Jiang-INS\", \"Jiang-TGFB\", \"Jiang-TNFA\",\n",
    "    \"Huang-HCT116\", \"Huang-HEK293T\",\n",
    "\n",
    "    \"Nadig-HEPG2\", \"Nadig-JURKAT\",\n",
    "\n",
    "    # double perturbation\n",
    "    \"Norman\", \"Wessels\",\n",
    "]\n",
    "\n",
    "path_origin = 'origin/'\n",
    "path_sc = 'sc/';os.makedirs(path_sc, exist_ok=True)\n",
    "path_bulk = 'bulk/';os.makedirs(path_bulk, exist_ok=True)\n",
    "path_bulk_log = 'bulk_log/';os.makedirs(path_bulk_log, exist_ok=True)\n",
    "\n",
    "dataset = \"Huang-HCT116\"\n",
    "print(f'Processing {dataset}...')\n",
    "\n",
    "adata = sc.read_h5ad(f'{path_origin}{dataset}.h5ad')\n",
    "print(adata)\n",
    "if dataset == 'Adamson':\n",
    "    adata.obs.rename({'gene':'perturbation'}, axis=1, inplace=True)\n",
    "    adata.obs.loc[:,'perturbation'] = adata.obs['perturbation'].astype(str).replace({'CTRL':'control'}).values\n",
    "    adata = adata[adata.obs['perturbation']!='None']\n",
    "elif dataset.startswith('Huang'):\n",
    "    adata.obs.rename({'gene_target':'perturbation'}, axis=1, inplace=True)\n",
    "    adata.obs.loc[:,'perturbation'] = adata.obs['perturbation'].astype(str).replace({'Non-Targeting':'control'}).values\n",
    "elif dataset.startswith('Nadig'):\n",
    "    adata.obs.rename({'gene':'perturbation'}, axis=1, inplace=True)\n",
    "    adata.obs['perturbation'] = adata.obs['perturbation'].astype(str)\n",
    "    adata.obs.loc[:,'perturbation'] = adata.obs['perturbation'].astype(str).replace({'non-targeting':'control'}).values\n",
    "    adata.var.set_index('gene_name', inplace=True)\n",
    "elif dataset.startswith('Jiang'):\n",
    "    raise NotImplementedError(\"Currently not supported for Jiang dataset.\")\n",
    "\n",
    "print(summarize(adata))\n",
    "\n",
    "# filter out cells\n",
    "if dataset.startswith('Huang'):\n",
    "    print('Skipping filtering cells for Huang datasets')\n",
    "    print('Min genes by counts:', adata.obs['n_genes_by_counts'].min())\n",
    "else:\n",
    "    sc.pp.filter_cells(adata, min_genes=100)\n",
    "print(summarize(adata))\n",
    "\n",
    "# filter perturbation condition\n",
    "ncells_pert = adata.obs.groupby('perturbation', observed=True).size()\n",
    "min_cells = 25 if dataset.startswith('Nadig') else 50\n",
    "valid_pert = np.array(ncells_pert[ncells_pert >= min_cells].index)\n",
    "valid_pert = valid_pert[np.isin(valid_pert, adata.var.index)]\n",
    "# TODO: filter inefficient perturbations\n",
    "adata = adata[adata.obs['perturbation'].isin(np.append(valid_pert, ['control']))]\n",
    "print(summarize(adata))\n",
    "\n",
    "\n",
    "# # filter cells by perturbation quantile effect\n",
    "# adata = filter_cells_by_pert_effect(adata)\n",
    "# print(summarize(adata))\n",
    "\n",
    "if not sp.sparse.issparse(adata.X):\n",
    "    adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "\n",
    "# Filter genes with less than 100 cells in the control groups, but keep those in adata.obs.index\n",
    "gene_filter = (np.sum(adata[adata.obs['perturbation'] == 'control'].X > 0, axis=0) >= 100) | adata.var.index.isin(valid_pert)\n",
    "adata = adata[:, gene_filter]\n",
    "\n",
    "duplicate_var_names = adata.var_names[adata.var_names.duplicated()]\n",
    "print(f\"Duplicate var names: {duplicate_var_names}\")\n",
    "adata = adata[:, ~adata.var_names.duplicated()]\n",
    "# sc.pp.filter_genes(adata, min_cells=100)\n",
    "# sc.pp.filter_genes(adata, max_counts=10) # this seems to be too strict\n",
    "print(summarize(adata))\n",
    "\n",
    "\n",
    "# Add information\n",
    "adata.obs.loc[:,'dataset'] = dataset\n",
    "# TODO: add celltype/pathway information\n",
    "# if 'pathway' not in adata.obs:\n",
    "if dataset.startswith('Huang') or dataset.startswith('Nadig'):\n",
    "    adata.obs.loc[:,'celltype'] = dataset.split('-')[1]    \n",
    "elif not dataset.startswith('Jiang'):\n",
    "    dict_cts = {\n",
    "        'Adamson': 'K562',\n",
    "        'Replogle-GW-k562': 'K562',\n",
    "        'Replogle-E-k562': 'K562',\n",
    "        'Replogle-E-rpe1': 'RPE1',\n",
    "        'Frangieh':'melanoma',\n",
    "        \"Tian-crispra\": 'iPSC', \n",
    "        \"Tian-crispri\": 'iPSC'\n",
    "        }\n",
    "    adata.obs.loc[:,'celltype'] = dict_cts[dataset]\n",
    "\n",
    "    adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "\n",
    "print(summarize(adata))\n",
    "\n",
    "\n",
    "col = adata.obs['perturbation'].astype(str)\n",
    "perts = sorted(p for p in col.unique() if p != 'control')\n",
    "# pert_path = \"/project/jingshuw/SeqExpDesign/vcc/data/pert_Huang-HCT116.csv\"\n",
    "# pd.Series(perts, name=\"perturbation\").to_csv(pert_path, index=False)\n",
    "print(\"The number of perturbations:\", len(perts))\n",
    "\n",
    "batch_size = 200\n",
    "batch_idx = int(sys.argv[1])\n",
    "start = (batch_idx - 1) * batch_size\n",
    "end = min(batch_idx * batch_size, len(perts))\n",
    "\n",
    "batch_perts = perts[start:end]\n",
    "print(f\"Batch {batch_idx}: {len(batch_perts)} perturbations ({start+1}-{end})\")\n",
    "\n",
    "for pert in batch_perts:\n",
    "    adata_split = adata[col.isin([pert, 'control'])].copy()\n",
    "    print(f\"[PAIR] pert={pert} | cells={adata_split.n_obs} | genes={adata_split.n_vars}\")\n",
    "    out_path = os.path.join(path_sc, dataset, f'{pert}.h5ad')\n",
    "    adata_split.write_h5ad(out_path)\n",
    "\n",
    "# # adata = sc.read_h5ad(f'{path_sc}{dataset}_overlap_vcc.h5ad')\n",
    "\n",
    "# ##############################################################################\n",
    "# #\n",
    "# # Save the bulk expression data\n",
    "# #\n",
    "# ##############################################################################\n",
    "# # print('Save the bulk expression data')\n",
    "\n",
    "# # For pseudo bulk aggregation\n",
    "# # Aggregate counts of adata.X according to perturbation\n",
    "# # adata_bulk = comp_bulk_expressions_batch(adata, key='perturbation')\n",
    "\n",
    "# # # calculate the library size\n",
    "# # adata_bulk.obs['n_counts'] = adata_bulk.X.sum(axis=1)\n",
    "\n",
    "# # print(adata_bulk)\n",
    "# # adata_bulk.write_h5ad(f'{path_bulk}{dataset}_overlap_vcc.h5ad')\n",
    "\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# #\n",
    "# # Save the bulk expression data (after log transformantion)\n",
    "# #\n",
    "# ##############################################################################\n",
    "# # print('Save the bulk expression data (after log transformantion)')\n",
    "\n",
    "\n",
    "# # For mean aggreagation\n",
    "# sc.pp.normalize_total(adata)\n",
    "# sc.pp.log1p(adata)\n",
    "\n",
    "# # # Aggregate counts of adata.X according to perturbation\n",
    "# # adata_bulk = comp_bulk_expressions_batch(adata, key='perturbation')\n",
    "\n",
    "# # # calculate the library size\n",
    "# # adata_bulk.obs['n_counts'] = adata_bulk.X.sum(axis=1)\n",
    "\n",
    "# # print(adata_bulk)\n",
    "# # adata_bulk.write_h5ad(f'{path_bulk_log}{dataset}_overlap_vcc.h5ad')\n",
    "\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# #\n",
    "# # Calculate DE genes from single-cell data\n",
    "# #\n",
    "# ##############################################################################\n",
    "# print('Calculate DE genes from single-cell data')\n",
    "\n",
    "# path_de = f'de/';os.makedirs(path_de, exist_ok=True)\n",
    "# data_dir = 'sc/'\n",
    "\n",
    "# def calculate_de_genes_group(adata, rankby_abs, test_name='t-test'):\n",
    "#     perturbations = adata.obs['perturbation'].unique()\n",
    "#     tie_correct = 'tie' if test_name == 'wilcoxon' else 't-test'\n",
    "#     sc.tl.rank_genes_groups(\n",
    "#         adata, groupby='perturbation', reference='control',\n",
    "#         n_genes=adata.shape[1], rankby_abs=rankby_abs=='abs',\n",
    "#         method=test_name, tie_correct=tie_correct=='tie',\n",
    "#         use_raw=False, n_jobs=njobs)\n",
    "#     df = pd.DataFrame(adata.uns['rank_genes_groups']['names']).T\n",
    "#     df_rank = df.apply(lambda x: pd.Series(x.index, index=x.values)[adata.var.index], axis=1).astype(int)\n",
    "#     df_rank = df_rank[sorted(df_rank.columns)]\n",
    "#     de_result = {\"perturbations\": perturbations, \"rank_genes_groups\": adata.uns['rank_genes_groups'], \"df_rank\": df_rank}\n",
    "#     return de_result\n",
    "\n",
    "\n",
    "# def calculate_de_genes(adata, path_de, dataset, test_name='t-test'):\n",
    "#     \"\"\"\n",
    "#     Calculate DE genes from single-cell data using different statistical tests.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     adata : anndata.AnnData\n",
    "#         The input AnnData object containing single-cell data.\n",
    "#     path_de : str\n",
    "#         The path where DE results will be saved.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     None\n",
    "#         Results are saved to disk as pickle files.\n",
    "#     \"\"\"\n",
    "#     cell_types = adata.obs['celltype'].unique()\n",
    "#     per = [p for p in adata.obs['perturbation'].astype(str).unique() if p != 'control']; assert len(per)==1; pert_name = per[0]\n",
    "#     for rankby_abs in ['abs']: #['abs', 'noabs']\n",
    "#         path = f'{path_de}{test_name}/{rankby_abs}/{dataset}/';os.makedirs(path, exist_ok=True)\n",
    "#         assert len(cell_types) == 1\n",
    "#         de_genes = calculate_de_genes_group(adata, rankby_abs, test_name=test_name)\n",
    "            \n",
    "#         with open(f'{path}{pert_name}.pkl', 'wb') as f:\n",
    "#             pickle.dump(de_genes, f)\n",
    "\n",
    "\n",
    "\n",
    "# for test_name in ['wilcoxon']:\n",
    "#     calculate_de_genes(adata, path_de, dataset, test_name=test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"11\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"11\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"11\" # export NUMEXPR_NUM_THREADS=6\n",
    "os.environ[\"NUMBA_CACHE_DIR\"]='/tmp/numba_cache'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "\n",
    "import pickle\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "import numba\n",
    "import multiprocessing\n",
    "# njobs = max(1, multiprocessing.cpu_count())\n",
    "njobs = min(1, multiprocessing.cpu_count())\n",
    "numba.set_num_threads(njobs)\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "print(pd.__version__, sc.__version__, ad.__version__)\n",
    "\n",
    "\n",
    "def summarize(adata):\n",
    "    N_C, N_G = adata.shape\n",
    "    N_P = adata.obs['perturbation'].nunique()\n",
    "    N_P_2 = adata.obs['perturbation'][adata.obs['perturbation'].str.contains('_')].nunique()\n",
    "    return N_C, N_G, N_P, N_P - N_P_2, N_P_2\n",
    "\n",
    "def comp_bulk_expressions(adata, key='perturbation'):\n",
    "    '''\n",
    "    Calculate the bulk expressions (in the log-scale) in the given DataFrame.\n",
    "    This function groups the DataFrame by the 'perturbation' column and then \n",
    "    applies a transformation to calculate the average effect. The transformation \n",
    "    involves taking logarithm of the mean of the exponentiated values minus one for each group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData or pandas.DataFrame\n",
    "        An AnnData or a DataFrame containing the data with a column 'key'.\n",
    "    key : str, optional\n",
    "        The column name to group by, default is 'condition'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata_bulk : anndata.AnnData or pandas.DataFrame\n",
    "        An AnnData or a DataFrame with the average effect of each perturbation.\n",
    "    '''\n",
    "    if isinstance(adata, ad.AnnData):\n",
    "        df = adata.to_df()\n",
    "        key_pert = adata.obs[key]        \n",
    "    else:\n",
    "        df = adata\n",
    "        key_pert = key\n",
    "    obs = adata.obs.drop_duplicates(subset=[key]).set_index(key).sort_values(key)\n",
    "    obs['n_cells'] = adata.obs.groupby(key, observed=True).size()\n",
    "    var = adata.var\n",
    "    # save memory by removing the original adata\n",
    "    # del adata\n",
    "    gc.collect()\n",
    "    df = df.astype(np.float32)  # Ensure consistent data type for calculations\n",
    "    grouped = df.groupby(key_pert, observed=True)\n",
    "    sums = grouped.sum().astype(np.float32)\n",
    "    sizes = grouped.size().astype(np.float32)\n",
    "    means = sums / sizes.values[:, None]\n",
    "    stds = grouped.std().astype(np.float32)\n",
    "    adata_bulk = means  # You can return or use both means and stds as needed\n",
    "    del df\n",
    "    gc.collect()\n",
    "    adata_bulk = ad.AnnData(\n",
    "        X=sp.sparse.csr_matrix(adata_bulk.values), \n",
    "        obs=obs, \n",
    "        var=var\n",
    "        )\n",
    "    return adata_bulk, stds\n",
    "\n",
    "def comp_bulk_expressions_batch(adata, key='perturbation', group_size=1000):\n",
    "    \"\"\"\n",
    "    Wrapper function to calculate bulk expressions in smaller groups to save memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        An AnnData object containing the data with a column 'key'.\n",
    "    key : str, optional\n",
    "        The column name to group by, default is 'perturbation'.\n",
    "    group_size : int, optional\n",
    "        Number of perturbation conditions to process in each group, default is 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adata_bulk : anndata.AnnData\n",
    "        An AnnData object with the average effect of each perturbation.\n",
    "    \"\"\"\n",
    "    perturbations = np.sort(np.array(adata.obs[key].unique()))\n",
    "    grouped_bulk = []\n",
    "    stds = []\n",
    "    for i in tqdm(range(0, len(perturbations), group_size), desc='Processing perturbations'):\n",
    "        subset_perturbations = perturbations[i:(i+group_size)]\n",
    "        sub_adata = adata[adata.obs[key].isin(subset_perturbations)]\n",
    "        if sub_adata.shape[0] == 0:\n",
    "            continue\n",
    "        sub_bulk, std = comp_bulk_expressions(sub_adata, key=key)\n",
    "        grouped_bulk.append(sub_bulk.copy())        \n",
    "        stds.append(std)\n",
    "    # Concatenate all sub-bulk results\n",
    "    adata_bulk = ad.concat(grouped_bulk, merge='same')\n",
    "    stds = pd.concat(stds, axis=0)\n",
    "    adata_bulk.uns['std'] = stds.copy()\n",
    "    return adata_bulk\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    # single perturbation\n",
    "    \"Adamson\", \"Frangieh\",    \n",
    "    \"Replogle-GW-k562\", \"Replogle-E-k562\", \"Replogle-E-rpe1\",\n",
    "    \"Tian-crispra\", \"Tian-crispri\",\n",
    "    \"Jiang-IFNB\", \"Jiang-IFNG\", \"Jiang-INS\", \"Jiang-TGFB\", \"Jiang-TNFA\",\n",
    "    \"Huang-HCT116\", \"Huang-HEK293T\",\n",
    "\n",
    "    \"Nadig-HEPG2\", \"Nadig-JURKAT\",\n",
    "\n",
    "    # double perturbation\n",
    "    \"Norman\", \"Wessels\",\n",
    "]\n",
    "\n",
    "path_origin = 'origin/'\n",
    "path_sc = 'sc/';os.makedirs(path_sc, exist_ok=True)\n",
    "path_bulk = 'bulk/';os.makedirs(path_bulk, exist_ok=True)\n",
    "path_bulk_log = 'bulk_log/';os.makedirs(path_bulk_log, exist_ok=True)\n",
    "\n",
    "dataset = \"Huang-HCT116\"\n",
    "print(f'Processing {dataset}...')\n",
    "\n",
    "# adata = sc.read_h5ad(f'{path_origin}{dataset}.h5ad')\n",
    "# print(adata)\n",
    "# if dataset == 'Adamson':\n",
    "#     adata.obs.rename({'gene':'perturbation'}, axis=1, inplace=True)\n",
    "#     adata.obs.loc[:,'perturbation'] = adata.obs['perturbation'].astype(str).replace({'CTRL':'control'}).values\n",
    "#     adata = adata[adata.obs['perturbation']!='None']\n",
    "# elif dataset.startswith('Huang'):\n",
    "#     adata.obs.rename({'gene_target':'perturbation'}, axis=1, inplace=True)\n",
    "#     adata.obs.loc[:,'perturbation'] = adata.obs['perturbation'].astype(str).replace({'Non-Targeting':'control'}).values\n",
    "# elif dataset.startswith('Nadig'):\n",
    "#     adata.obs.rename({'gene':'perturbation'}, axis=1, inplace=True)\n",
    "#     adata.obs['perturbation'] = adata.obs['perturbation'].astype(str)\n",
    "#     adata.obs.loc[:,'perturbation'] = adata.obs['perturbation'].astype(str).replace({'non-targeting':'control'}).values\n",
    "#     adata.var.set_index('gene_name', inplace=True)\n",
    "# elif dataset.startswith('Jiang'):\n",
    "#     raise NotImplementedError(\"Currently not supported for Jiang dataset.\")\n",
    "\n",
    "# print(summarize(adata))\n",
    "\n",
    "# # filter out cells\n",
    "# if dataset.startswith('Huang'):\n",
    "#     print('Skipping filtering cells for Huang datasets')\n",
    "#     print('Min genes by counts:', adata.obs['n_genes_by_counts'].min())\n",
    "# else:\n",
    "#     sc.pp.filter_cells(adata, min_genes=100)\n",
    "# print(summarize(adata))\n",
    "\n",
    "# # filter perturbation condition\n",
    "# ncells_pert = adata.obs.groupby('perturbation', observed=True).size()\n",
    "# min_cells = 25 if dataset.startswith('Nadig') else 50\n",
    "# valid_pert = np.array(ncells_pert[ncells_pert >= min_cells].index)\n",
    "# valid_pert = valid_pert[np.isin(valid_pert, adata.var.index)]\n",
    "# # TODO: filter inefficient perturbations\n",
    "# adata = adata[adata.obs['perturbation'].isin(np.append(valid_pert, ['control']))]\n",
    "# print(summarize(adata))\n",
    "\n",
    "\n",
    "# # # filter cells by perturbation quantile effect\n",
    "# # adata = filter_cells_by_pert_effect(adata)\n",
    "# # print(summarize(adata))\n",
    "\n",
    "# if not sp.sparse.issparse(adata.X):\n",
    "#     adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "\n",
    "# # Filter genes with less than 100 cells in the control groups, but keep those in adata.obs.index\n",
    "# gene_filter = (np.sum(adata[adata.obs['perturbation'] == 'control'].X > 0, axis=0) >= 100) | adata.var.index.isin(valid_pert)\n",
    "# adata = adata[:, gene_filter]\n",
    "\n",
    "# duplicate_var_names = adata.var_names[adata.var_names.duplicated()]\n",
    "# print(f\"Duplicate var names: {duplicate_var_names}\")\n",
    "# adata = adata[:, ~adata.var_names.duplicated()]\n",
    "# # sc.pp.filter_genes(adata, min_cells=100)\n",
    "# # sc.pp.filter_genes(adata, max_counts=10) # this seems to be too strict\n",
    "# print(summarize(adata))\n",
    "\n",
    "\n",
    "# # Add information\n",
    "# adata.obs.loc[:,'dataset'] = dataset\n",
    "# # TODO: add celltype/pathway information\n",
    "# # if 'pathway' not in adata.obs:\n",
    "# if dataset.startswith('Huang') or dataset.startswith('Nadig'):\n",
    "#     adata.obs.loc[:,'celltype'] = dataset.split('-')[1]    \n",
    "# elif not dataset.startswith('Jiang'):\n",
    "#     dict_cts = {\n",
    "#         'Adamson': 'K562',\n",
    "#         'Replogle-GW-k562': 'K562',\n",
    "#         'Replogle-E-k562': 'K562',\n",
    "#         'Replogle-E-rpe1': 'RPE1',\n",
    "#         'Frangieh':'melanoma',\n",
    "#         \"Tian-crispra\": 'iPSC', \n",
    "#         \"Tian-crispri\": 'iPSC'\n",
    "#         }\n",
    "#     adata.obs.loc[:,'celltype'] = dict_cts[dataset]\n",
    "\n",
    "#     adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "\n",
    "# print(summarize(adata))\n",
    "\n",
    "# col = adata.obs['perturbation'].astype(str)\n",
    "# perts = sorted(p for p in col.unique() if p != 'control')\n",
    "# print(\"The number of perturbations:\", len(perts))\n",
    "\n",
    "# batch_size = 100\n",
    "# batch_idx = int(sys.argv[1])\n",
    "# start = (batch_idx - 1) * batch_size\n",
    "# end = min(batch_idx * batch_size, len(perts))\n",
    "\n",
    "# batch_perts = perts[start:end]\n",
    "# print(f\"Batch {batch_idx}: {len(batch_perts)} perturbations ({start+1}-{end})\")\n",
    "\n",
    "# for pert in batch_perts:\n",
    "#     adata_split = adata[col.isin([pert, 'control'])].copy()\n",
    "#     print(f\"[PAIR] pert={pert} | cells={adata_split.n_obs} | genes={adata_split.n_vars}\")\n",
    "#     out_path = os.path.join(path_sc, dataset, f'{pert}.h5ad')\n",
    "#     adata_split.write_h5ad(out_path)\n",
    "\n",
    "# adata = sc.read_h5ad(f'{path_sc}{dataset}_overlap_vcc.h5ad')\n",
    "\n",
    "pert_path = f\"./pert_{dataset}.csv\"\n",
    "perts = pd.read_csv(pert_path)[\"perturbation\"].tolist()\n",
    "batch_idx = int(sys.argv[1])\n",
    "pert = perts[batch_idx - 1]\n",
    "print(f\"Batch {batch_idx}: loading perturbation '{pert}'\")\n",
    "\n",
    "adata_path = os.path.join(path_sc, dataset, f\"{pert}.h5ad\")\n",
    "\n",
    "print(f\"Batch {batch_idx}: loading {adata_path}\")\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "print(f\"[LOAD] pert={pert} | cells={adata.n_obs} | genes={adata.n_vars}\")\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Save the bulk expression data\n",
    "#\n",
    "##############################################################################\n",
    "# print('Save the bulk expression data')\n",
    "\n",
    "# For pseudo bulk aggregation\n",
    "# Aggregate counts of adata.X according to perturbation\n",
    "# adata_bulk = comp_bulk_expressions_batch(adata, key='perturbation')\n",
    "\n",
    "# # calculate the library size\n",
    "# adata_bulk.obs['n_counts'] = adata_bulk.X.sum(axis=1)\n",
    "\n",
    "# print(adata_bulk)\n",
    "# adata_bulk.write_h5ad(f'{path_bulk}{dataset}_overlap_vcc.h5ad')\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Save the bulk expression data (after log transformantion)\n",
    "#\n",
    "##############################################################################\n",
    "# print('Save the bulk expression data (after log transformantion)')\n",
    "\n",
    "\n",
    "# For mean aggreagation\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# # Aggregate counts of adata.X according to perturbation\n",
    "# adata_bulk = comp_bulk_expressions_batch(adata, key='perturbation')\n",
    "\n",
    "# # calculate the library size\n",
    "# adata_bulk.obs['n_counts'] = adata_bulk.X.sum(axis=1)\n",
    "\n",
    "# print(adata_bulk)\n",
    "# adata_bulk.write_h5ad(f'{path_bulk_log}{dataset}_overlap_vcc.h5ad')\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Calculate DE genes from single-cell data\n",
    "#\n",
    "##############################################################################\n",
    "print('Calculate DE genes from single-cell data')\n",
    "\n",
    "path_de = f'de/';os.makedirs(path_de, exist_ok=True)\n",
    "data_dir = 'sc/'\n",
    "\n",
    "def calculate_de_genes_group(adata, rankby_abs, test_name='t-test'):\n",
    "    perturbations = adata.obs['perturbation'].unique()\n",
    "    tie_correct = 'tie' if test_name == 'wilcoxon' else 't-test'\n",
    "    sc.tl.rank_genes_groups(\n",
    "        adata, groupby='perturbation', reference='control',\n",
    "        n_genes=adata.shape[1], rankby_abs=rankby_abs=='abs',\n",
    "        method=test_name, tie_correct=tie_correct=='tie',\n",
    "        use_raw=False, n_jobs=njobs)\n",
    "    df = pd.DataFrame(adata.uns['rank_genes_groups']['names']).T\n",
    "    df_rank = df.apply(lambda x: pd.Series(x.index, index=x.values)[adata.var.index], axis=1).astype(int)\n",
    "    df_rank = df_rank[sorted(df_rank.columns)]\n",
    "    de_result = {\"perturbations\": perturbations, \"rank_genes_groups\": adata.uns['rank_genes_groups'], \"df_rank\": df_rank}\n",
    "    return de_result\n",
    "\n",
    "\n",
    "def calculate_de_genes(adata, path_de, dataset, test_name='t-test'):\n",
    "    \"\"\"\n",
    "    Calculate DE genes from single-cell data using different statistical tests.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata.AnnData\n",
    "        The input AnnData object containing single-cell data.\n",
    "    path_de : str\n",
    "        The path where DE results will be saved.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Results are saved to disk as pickle files.\n",
    "    \"\"\"\n",
    "    cell_types = adata.obs['celltype'].unique()\n",
    "    per = [p for p in adata.obs['perturbation'].astype(str).unique() if p != 'control']; assert len(per)==1; pert_name = per[0]\n",
    "    for rankby_abs in ['abs']: #['abs', 'noabs']\n",
    "        path = f'{path_de}{test_name}/{rankby_abs}/{dataset}/';os.makedirs(path, exist_ok=True)\n",
    "        assert len(cell_types) == 1\n",
    "        de_genes = calculate_de_genes_group(adata, rankby_abs, test_name=test_name)\n",
    "            \n",
    "        with open(f'{path}{pert_name}.pkl', 'wb') as f:\n",
    "            pickle.dump(de_genes, f)\n",
    "\n",
    "\n",
    "\n",
    "for test_name in ['wilcoxon']:\n",
    "    calculate_de_genes(adata, path_de, dataset, test_name=test_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
